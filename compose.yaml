name: maird
services:
    vllm-openai:
        runtime: nvidia
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        volumes:
            - ${PWD}/.cache/huggingface:/root/.cache/huggingface
        ports:
            - 8791:8000
        env_file:
            - path: ${PWD}/vllm/.env
        image: vllm/vllm-openai:v0.6.3
        command: --model meta-llama/Llama-3.2-1B-Instruct --max-model-len=512 --gpu-memory-utilization=0.6
        healthcheck:
          test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
          start_period: 60s
          interval: 30s
          timeout: 10s
          retries: 10
    infinity-embedding:
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        volumes:
            - ${PWD}/.cache:/app/.cache
        ports:
            - 8792:7997
        image: michaelf34/infinity:0.0.70-nvidia-torch25
        command: v2 --model-id=jinaai/jina-embeddings-v3 --url-prefix=/v1
        healthcheck:
          test: ["CMD", "curl", "-f", "http://localhost:7997/health"]
          start_period: 60s
          interval: 30s
          timeout: 10s
          retries: 10
    infinity-rerank:
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
        volumes:
            - ${PWD}/.cache:/app/.cache
        ports:
            - 8793:7997
        image: michaelf34/infinity:0.0.70-nvidia-torch25
        command: v2 --model-id=jinaai/jina-reranker-v2-base-multilingual --url-prefix=/v2
        healthcheck:
          test: ["CMD", "curl", "-f", "http://localhost:7997/health"]
          start_period: 60s
          interval: 30s
          timeout: 10s
          retries: 10
    litellm:
        volumes:
            - ${PWD}/litellm/config.yaml:/app/config.yaml
        ports:
            - 8800:4000
        image: ghcr.io/berriai/litellm:main-v1.52.14
        command: --config /app/config.yaml --detailed_debug
        depends_on:
          vllm-openai:
            condition: service_healthy
          infinity-embedding:
            condition: service_healthy
          infinity-rerank:
            condition: service_healthy